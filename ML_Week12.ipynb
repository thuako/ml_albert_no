{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12\n",
    "\n",
    "VGG13 for CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0th epoch starting.\n",
      "Epoch [1] Loss: 2.2744\n",
      "[Test set] Average loss: 0.0091, Accuracy: 1302/10000 (13.02%)\n",
      "\n",
      "Time ellapsed in training is: 23.93739676475525\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'mkdirs'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-233d2a4ca049>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG13\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_param_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-d6a32084da01>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(hyper_param_dict, model)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'save dir : {save_dir}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'mkdirs'"
     ]
    }
   ],
   "source": [
    "model = VGG13()\n",
    "\n",
    "train(hyper_param_dict, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#hyper parameter \n",
    "\n",
    "hyper_param_dict = { \n",
    "                'project' : 'VGG13.v1',\n",
    "                'epochs' : 1,\n",
    "                'batch' : 256, \n",
    "                'lr' : 0.05,\n",
    "                'optimizer': 'SGD',\n",
    "                'momentum' : 0.9,   \n",
    "                'beta1' : 0.9,\n",
    "                'beta2' : 0.999, \n",
    "                'weight_decay' : 5e-4\n",
    "                }\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "'''\n",
    "Step 1:\n",
    "'''\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=hp_dataset,\n",
    "                                 train=True, \n",
    "                                 transform=transform,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=hp_dataset,\n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nStep 3\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Step 2\n",
    "'''\n",
    "\n",
    "class VGG13(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(VGG13, self).__init__()\n",
    "        \n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, padding=1),     # 64 * 32 * 32\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, kernel_size=3, padding=1),    # 64 * 32 * 32\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)           # 64 * 16 * 16\n",
    "                )\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1),   # 128 * 16 * 16\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, kernel_size=3, padding=1),  # 128 * 16 * 16\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)           # 128 * 8 * 8\n",
    "                )\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "                nn.Conv2d(128, 256, kernel_size=3, padding=1),  # 256 * 8 * 8\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(256, 256, kernel_size=3, padding=1),  # 256 * 8 * 8\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(256, 256, kernel_size=3, padding=1),  # 256 * 8 * 8\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)           # 256 * 4 * 4\n",
    "                )\n",
    "        self.conv_layer4 = nn.Sequential(\n",
    "                nn.Conv2d(256, 512, kernel_size=3, padding=1),  # 512 * 4 * 4\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1),  # 512 * 4 * 4\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1),  # 512 * 4 * 4\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),          # 512 * 2 * 2\n",
    "                )\n",
    "        self.fc_layer1 = nn.Sequential(\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(512*2*2, 4096),                           # 1 * 4096\n",
    "                nn.ReLU()\n",
    "                )\n",
    "        self.fc_layer2 = nn.Sequential(\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),                          # 1 * 4096\n",
    "                nn.ReLU()\n",
    "                )\n",
    "        self.fc_layer3 = nn.Sequential(\n",
    "                nn.Linear(4096, 10),                     # 1 * num_class\n",
    "                )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        output = self.conv_layer1(x)\n",
    "        output = self.conv_layer2(output)\n",
    "        output = self.conv_layer3(output)\n",
    "        output = self.conv_layer4(output)\n",
    "        output = output.view(-1, 512*2*2)\n",
    "        output = self.fc_layer1(output)\n",
    "        output = self.fc_layer2(output)\n",
    "        output = self.fc_layer3(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "'''\n",
    "Step 3\n",
    "'''\n",
    "# model = VGG13().to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-77-724b947c1447>, line 96)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-77-724b947c1447>\"\u001b[0;36m, line \u001b[0;32m96\u001b[0m\n\u001b[0;31m    os.make dirs(save_dir)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train Fuction\n",
    "'''\n",
    "\n",
    "def train(hyper_param_dict, model):    \n",
    "\n",
    "    model.to(device)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    if hyper_param_dict['optimizer'] == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=hyper_param_dict['lr'], betas=(hyper_param_dict['beta1'], hyper_param_dict['beta2']), \n",
    "                                weight_decay=hyper_param_dict['weight_decay'])\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=hyper_param_dict['lr'], momentum=hyper_param_dict['momentum'], \n",
    "                                weight_decay=hyper_param_dict['weight_decay'])\n",
    "\n",
    "    model.train()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=hyper_param_dict['batch'], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=hyper_param_dict['batch'], shuffle=False)\n",
    "\n",
    "    train_acc_list, test_acc_list, train_loss_list, test_loss_list = [], [], [], []\n",
    "\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for epoch in range(hyper_param_dict['epochs']) :\n",
    "            print(\"{}th epoch starting.\".format(epoch))\n",
    "\n",
    "            # start training\n",
    "            model.train()\n",
    "            train_loss, train_acc, total, correct = 0, 0, 0, 0\n",
    "            for i, (images, labels) in enumerate(train_loader) :\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(images)\n",
    "                    train_loss = loss_function(output, labels)\n",
    "                    train_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pred = output.max(1, keepdim=True)[1]\n",
    "                    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            print (\"Epoch [{}] Loss: {:.4f}\".format(epoch+1, train_loss.item()))\n",
    "            #save train result\n",
    "            train_loss_list.append(train_loss / total)\n",
    "            train_acc_list.append(correct / total * 100.)\n",
    "\n",
    "            # start evaluation\n",
    "            model.eval()    \n",
    "            test_loss, correct, total = 0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                    for images, labels in test_loader :\n",
    "                            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                            output = model(images)\n",
    "                            test_loss += loss_function(output, labels).item()\n",
    "\n",
    "                            pred = output.max(1, keepdim=True)[1]\n",
    "                            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "                            total += labels.size(0)\n",
    "\n",
    "            print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                    test_loss /total, correct, total,\n",
    "                    100. * correct / total))\n",
    "            #save test result\n",
    "            train_loss_list.append(train_loss / total)\n",
    "            train_acc_list.append(correct / total * 100.)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time ellapsed in training is: {}\".format(end - start))\n",
    "\n",
    "    hyper_param_dict['training time'] = end - start\n",
    "\n",
    "    '''\n",
    "    Step 5\n",
    "    '''\n",
    "    # save file by numpy\n",
    "    np_train_acc_list, np_test_acc_list = np.array(train_acc_list), np.array(test_acc_list)\n",
    "    np_train_loss_list, np_test_loss_list = np.array(train_loss_list), np.array(test_loss_list)\n",
    "\n",
    "    if not os.path.isdir('./Result'):\n",
    "            os.makedirs('./Result')\n",
    "\n",
    "    # Make save directory\n",
    "\n",
    "    project_name = hyper_param_dict['project']\n",
    "\n",
    "    now = time.localtime()\n",
    "    save_time = str(now.tm_mday) + '_' + str(now.tm_hour) + '_' + str(now.tm_min)\n",
    "    if project_name is not None:\n",
    "        save_dir = os.path.join(os.getcwd(), 'Result', project_name ,save_time)\n",
    "    else:\n",
    "        save_dir = os.path.join(os.getcwd(), 'Result', save_time)\n",
    "\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(f'save dir : {save_dir}')\n",
    "    np.save(save_dir + '/np_train_acc_list', np_train_acc_list)\n",
    "    np.save(save_dir + '/np_train_loss_list', np_train_loss_list)\n",
    "    np.save(save_dir + '/np_test_acc_list', np_test_acc_list)\n",
    "    np.save(save_dir + '/np_test_loss_list', np_test_loss_list)\n",
    "\n",
    "\n",
    "    hyper_param_dict = { 'epochs' : hp_epochs,\n",
    "                    'batch' : hp_batch, \n",
    "                    'lr' : hp_lr, \n",
    "                    'momentum' : hp_momentum, \n",
    "                    'weight_decay' : hp_weight_decay,\n",
    "                    'training time' : end - start}\n",
    "\n",
    "    with open( save_dir + '/hyper.pickle','wb') as fw:\n",
    "        pickle.dump(hyper_param_dict, fw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "'''\n",
    "Step 5\n",
    "'''\n",
    "# save file by numpy\n",
    "np_train_acc_list, np_test_acc_list = np.array(train_acc_list), np.array(test_acc_list)\n",
    "np_train_loss_list, np_test_loss_list = np.array(train_loss_list), np.array(test_loss_list)\n",
    "\n",
    "if not os.path.isdir('./Result'):\n",
    "        os.mkdir('./Result')\n",
    "\n",
    "# Make save directory\n",
    "\n",
    "project_name = hyper_param_dict['project']\n",
    "\n",
    "now = time.localtime()\n",
    "save_time = str(now.tm_mday) + '_' + str(now.tm_hour) + '_' + str(now.tm_min)\n",
    "if project_name is not None:\n",
    "    save_dir = os.path.join(os.getcwd(), 'Result', project_name ,save_time)\n",
    "else:\n",
    "    save_dir = os.path.join(os.getcwd(), 'Result', save_time)\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "print(f'save dir : {save_dir}')\n",
    "np.save(save_dir + '/np_train_acc_list', np_train_acc_list)\n",
    "np.save(save_dir + '/np_train_loss_list', np_train_loss_list)\n",
    "np.save(save_dir + '/np_test_acc_list', np_test_acc_list)\n",
    "np.save(save_dir + '/np_test_loss_list', np_test_loss_list)\n",
    "\n",
    "\n",
    "hyper_param_dict = { 'epochs' : hp_epochs,\n",
    "                'batch' : hp_batch, \n",
    "                'lr' : hp_lr, \n",
    "                'momentum' : hp_momentum, \n",
    "                'weight_decay' : hp_weight_decay,\n",
    "                'training time' : end - start}\n",
    "\n",
    "with open( save_dir + '/hyper.pickle','wb') as fw:\n",
    "    pickle.dump(hyper_param_dict, fw)\n",
    "\n",
    "'''\n",
    "with open(save_dir + '/hyper_parameter.txt', 'w') as f:\n",
    "        f.writelines([  'epochs : ' + str(hp_epochs),\n",
    "                        '\\nbatch : ' + str(hp_batch), \n",
    "                        '\\nlr : ' + str(hp_lr), \n",
    "                        '\\nmomentum : ' + str(hp_momentum), \n",
    "                        '\\nweight_decay : ' + str(hp_weight_decay),\n",
    "                        '\\ntraining time : ' + str(end - start)])'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print graphf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#find save dir path\n",
    "if project_name is not None:\n",
    "    save_dir = './Result/' + project_name\n",
    "else:\n",
    "    save_dir = './Result/'\n",
    "dir_list = os.listdir(save_dir)\n",
    "\n",
    "for dir_name in dir_list:\n",
    "    if '_' in dir_name:\n",
    "        load_dir = save_dir + dir_name\n",
    "        train_acc = np.load(load_dir + '/np_train_acc_list.npy', allow_pickle=True)\n",
    "        test_acc = np.load(load_dir + '/np_test_acc_list.npy', allow_pickle=True)\n",
    "        train_loss = np.load(load_dir + '/np_train_loss_list.npy', allow_pickle=True)\n",
    "        test_loss = np.load(load_dir + '/np_test_loss_list.npy', allow_pickle=True)\n",
    "\n",
    "        with open( load_dir + '/hyper.pickle', 'rb') as fr:\n",
    "            hyper_param_load = pickle.load(fr)\n",
    "\n",
    "        plt.plot(np.arange(0, train_acc.shape[0]), train_acc, label=  'lr = ' + str(hyper_param_load['lr']))\n",
    "        # plt.legend(str(float(hyper_param_load['lr'])) )\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet for CIFAR10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "'''\n",
    "Step 1:\n",
    "'''\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./cifar_10data/',\n",
    "                                 train=True, \n",
    "                                 transform=transform,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./cifar_10data/',\n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "    \n",
    "'''\n",
    "Step 2:\n",
    "'''\n",
    "class GoogLeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()        \n",
    "\n",
    "        self.conv1 = BasicConv2d(3, 64, kernel_size=7, padding=3)\n",
    "        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\n",
    "        self.conv3 = BasicConv2d(64, 192, kernel_size=5)\n",
    "\n",
    "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)  # 64 + 128 + 32 + 32 = 256\n",
    "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024, 10)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N x 3 x 32 x 32\n",
    "        x = self.conv1(x)\n",
    "        # N x 64 x 32 x 32\n",
    "        x = self.conv2(x)\n",
    "        # N x 64 x 32 x 32\n",
    "        x = self.conv3(x)\n",
    "        # N x 192 x 28 x 28\n",
    "\n",
    "        # N x 192 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        x = self.inception3b(x)\n",
    "        # N x 480 x 28 x 28\n",
    "        x = self.maxpool3(x)\n",
    "        # N x 480 x 14 x 14\n",
    "        x = self.inception4a(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4b(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4c(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4d(x)\n",
    "        # N x 528 x 14 x 14\n",
    "        x = self.inception4e(x)\n",
    "        # N x 832 x 14 x 14\n",
    "        x = self.maxpool4(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5a(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5b(x)\n",
    "        # N x 1024 x 7 x 7\n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        # N x 10 (num_classes)\n",
    "        return x\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
    "        super(Inception, self).__init__()\n",
    "        \n",
    "        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, **kwargs),\n",
    "                            nn.ReLU()\n",
    "                            )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "Step 3\n",
    "'''\n",
    "model = GoogLeNet().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "'''\n",
    "Step 4\n",
    "'''\n",
    "model.train()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(100) :\n",
    "    print(\"{}th epoch starting.\".format(epoch))\n",
    "    for i, (images, labels) in enumerate(train_loader) :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss_function(model(images), labels)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print (\"Epoch [{}] Loss: {:.4f}\".format(epoch+1, train_loss.item()))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time ellapsed in training is: {}\".format(end - start))\n",
    "\n",
    "\n",
    "'''\n",
    "Step 5\n",
    "'''\n",
    "model.eval()\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "with torch.no_grad():  #using context manager\n",
    "    for images, labels in test_loader :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        output = model(images)\n",
    "        test_loss += loss_function(output, labels).item()\n",
    "\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /total, correct, total,\n",
    "        100. * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet with BatchNorm CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "'''\n",
    "Step 1:\n",
    "'''\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./cifar_10data/',\n",
    "                                 train=True, \n",
    "                                 transform=transform,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./cifar_10data/',\n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "    \n",
    "'''\n",
    "Step 2:\n",
    "'''\n",
    "class GoogLeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()        \n",
    "\n",
    "        self.conv1 = BasicConv2d(3, 64, kernel_size=7, padding=3)\n",
    "        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\n",
    "        self.conv3 = BasicConv2d(64, 192, kernel_size=5)\n",
    "\n",
    "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024, 10)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
    "        super(Inception, self).__init__()\n",
    "        \n",
    "        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, **kwargs),\n",
    "                            nn.BatchNorm2d(out_channels),   #Batch norm here\n",
    "                            nn.ReLU()\n",
    "                            )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "Step 3\n",
    "'''\n",
    "model = GoogLeNet().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "'''\n",
    "Step 4\n",
    "'''\n",
    "model.train()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(100) :\n",
    "    print(\"{}th epoch starting.\".format(epoch))\n",
    "    for i, (images, labels) in enumerate(train_loader) :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss_function(model(images), labels)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print (\"Epoch [{}] Loss: {:.4f}\".format(epoch+1, train_loss.item()))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time ellapsed in training is: {}\".format(end - start))\n",
    "\n",
    "\n",
    "'''\n",
    "Step 5\n",
    "'''\n",
    "model.eval()\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "with torch.no_grad():  #using context manager\n",
    "    for images, labels in test_loader :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        output = model(images)\n",
    "        test_loss += loss_function(output, labels).item()\n",
    "\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /total, correct, total,\n",
    "        100. * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}